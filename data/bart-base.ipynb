{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c6edbe-9c90-445a-9bcc-a42b64b9b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86cbed9-f86c-40c3-8c0a-809789039a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"clean.csv\")  # should contain 'text' and 'title' columns\n",
    "data = data.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Optional: rename columns for clarity\n",
    "data = data.rename(columns={\"text\": \"article\", \"title\": \"headline\"})\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "raw_dataset = Dataset.from_pandas(data)\n",
    "dataset = raw_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3114db63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3354"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9620186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ioan Gruffudd dealt new blow in bitter divorce...</td>\n",
       "      <td>A month ago, Ioan Gruffudd revealed shocking t...</td>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was on Love Is Blind UK and even got engaged...</td>\n",
       "      <td>A Love Is Blind UK star revealed that they got...</td>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/tv/artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emotional Zoe Ball reveals how gardening helpe...</td>\n",
       "      <td>Zoe Ball has opened up about how gardening hel...</td>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Travolta honors late wife while Emily Rat...</td>\n",
       "      <td>John Travolta honored his late wife Kelly Pres...</td>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/tvshowbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Margot Robbie stuns in a corset mini dress as ...</td>\n",
       "      <td>Margot Robbie looked stunning as she joined he...</td>\n",
       "      <td>https://www.dailymail.co.uk/tvshowbiz/article-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Ioan Gruffudd dealt new blow in bitter divorce...   \n",
       "1  I was on Love Is Blind UK and even got engaged...   \n",
       "2  Emotional Zoe Ball reveals how gardening helpe...   \n",
       "3  John Travolta honors late wife while Emily Rat...   \n",
       "4  Margot Robbie stuns in a corset mini dress as ...   \n",
       "\n",
       "                                             article  \\\n",
       "0  A month ago, Ioan Gruffudd revealed shocking t...   \n",
       "1  A Love Is Blind UK star revealed that they got...   \n",
       "2  Zoe Ball has opened up about how gardening hel...   \n",
       "3  John Travolta honored his late wife Kelly Pres...   \n",
       "4  Margot Robbie looked stunning as she joined he...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.dailymail.co.uk/tvshowbiz/article-...  \n",
       "1  https://www.dailymail.co.uk/tvshowbiz/tv/artic...  \n",
       "2  https://www.dailymail.co.uk/tvshowbiz/article-...  \n",
       "3  https://www.dailymail.co.uk/tvshowbiz/tvshowbi...  \n",
       "4  https://www.dailymail.co.uk/tvshowbiz/article-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac6a69b-f5f5-4186-b388-8bd9636d43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Load Tokenizer and Model\n",
    "# ------------------------------------------\n",
    "model_checkpoint = \"facebook/bart-base\"  # or try \"google/pegasus-xsum\", \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d87e79-ebbc-4417-946c-5e334ecc99c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4c93a1e8a4947a8cb16fe5e4f704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3018 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a611409fa48c5bc76b03660f09cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Preprocessing\n",
    "# ------------------------------------------\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "prefix = \"generate clickbait headline: \"\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [prefix + article for article in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(examples[\"headline\"], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55db106f-25ca-4ee5-ae4f-a2443bf17c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Metrics for Evaluation\n",
    "# ------------------------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bcc937-cd79-4317-af74-81fd279ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./clickbait_model\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "\n",
    "    # core\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # logging (force first-step log and visible tqdm)\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    disable_tqdm=False,          # <-- show live loss from tqdm\n",
    "\n",
    "    # evaluation (your version uses eval_steps directly)\n",
    "    eval_steps=500,\n",
    "\n",
    "    # safeguards that also fix several zero-loss logging cases\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "    # MPS nicety\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # misc\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4fcf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/trdqjzps4cbdpc2xtc8r53sm0000gn/T/ipykernel_39342/3608310339.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1134' max='1134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1134/1134 16:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.946600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.776000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weiling/anaconda3/envs/clickbait/lib/python3.10/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1134, training_loss=2.1531106088527294, metrics={'train_runtime': 1012.883, 'train_samples_per_second': 8.939, 'train_steps_per_second': 1.12, 'total_flos': 2760276946452480.0, 'train_loss': 2.1531106088527294, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8306bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual batch loss: nan\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "model.train()\n",
    "dl = DataLoader(tokenized_dataset[\"train\"], batch_size=8,\n",
    "                collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model))\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(**{k: v.to(model.device) for k, v in batch.items() if k in [\"input_ids\",\"attention_mask\",\"labels\",\"decoder_input_ids\"]})\n",
    "print(\"manual batch loss:\", float(out.loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2361d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': (torch.Size([8, 512]), torch.int64), 'attention_mask': (torch.Size([8, 512]), torch.int64), 'labels': (torch.Size([8, 50]), torch.int64), 'decoder_input_ids': (torch.Size([8, 50]), torch.int64)}\n",
      "labels all -100 per example: [False, False, False, False, False, False, False, False]\n",
      "fraction of tokens kept in labels: 0.575\n"
     ]
    }
   ],
   "source": [
    "# 1) Peek a real batch and make sure labels aren’t all -100\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(tokenized_dataset[\"train\"], batch_size=8, collate_fn=data_collator)\n",
    "batch = next(iter(dl))\n",
    "print({k: (v.shape, v.dtype) for k, v in batch.items()})\n",
    "import torch\n",
    "all_ignored_per_example = (batch[\"labels\"] == -100).all(dim=1)\n",
    "print(\"labels all -100 per example:\", all_ignored_per_example.tolist())\n",
    "print(\"fraction of tokens kept in labels:\",\n",
    "      float((batch[\"labels\"] != -100).sum()) / batch[\"labels\"].numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18425e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3a474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ee5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06087703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6339671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbd54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d534820-43b3-4033-8eed-caa4676b169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/trdqjzps4cbdpc2xtc8r53sm0000gn/T/ipykernel_40051/3608310339.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/Users/weiling/anaconda3/envs/clickbait/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 03:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.816800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.641400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weiling/anaconda3/envs/clickbait/lib/python3.10/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=216, training_loss=2.168049344310054, metrics={'train_runtime': 192.0611, 'train_samples_per_second': 8.997, 'train_steps_per_second': 1.125, 'total_flos': 526812299919360.0, 'train_loss': 2.168049344310054, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ba4f55-5152-4d47-bd79-02dfc8298dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./clickbait_model/tokenizer_config.json',\n",
       " './clickbait_model/special_tokens_map.json',\n",
       " './clickbait_model/vocab.json',\n",
       " './clickbait_model/merges.txt',\n",
       " './clickbait_model/added_tokens.json',\n",
       " './clickbait_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Save Model and Tokenizer\n",
    "# ------------------------------------------\n",
    "trainer.save_model(\"./clickbait_model\")\n",
    "tokenizer.save_pretrained(\"./clickbait_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6437598-65c7-408c-8bd1-8c8ac97f170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clickbait(article_text):\n",
    "    input_text = prefix + article_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "    model.to(\"cpu\")\n",
    "    outputs = model.generate(**inputs, max_length=max_target_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ff7684-5dbb-44f6-8a8b-72c6c6988145",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Country Road is fighting for its future largely thanks to a cost-cutting decision the company made more than 20 years ago, an expert says. The once-beloved Aussie brand is in clear trouble, with Country Road Group reporting in March its earnings were down almost 72 per cent at $14.2million for the last half of 2024.\n",
    "\n",
    "One of its longstanding flagship stores at Sydney CBD's Queen Victoria Building has shut up shop, as has sister brand Trenery in Mosman, on Sydney's affluent lower north shore. Another CBD store in Sydney's Pitt Street Mall is expected to close when its lease expires in three years' time.\n",
    "\n",
    "The video player is currently playing an ad. You can skip the ad in 5 sec with a mouse or keyboard\n",
    "Queensland University of Technology marketing expert Gary Mortimer said Country Road had lost its iconic Australian lifestyle brand status since Woolworths Holdings took a controlling stake in the late 90s.\n",
    "\n",
    "A cost-cutting move to manufacture offshore gradually eroded its 'Made in Australia' appeal and weakened the brand's authenticity, Professor Mortimer said.\n",
    "\n",
    "'Since its launch in the mid-1970s, Country Road clothing was primarily made in Australia, specifically, the iconic chambray shirt which I and nearly every other young man had during that time was made here using Australian cotton,' he said.\n",
    "\n",
    "'The company emphasised its commitment to Australian manufacturing during that time. \n",
    "\n",
    "'Much of that production has shifted to Bangladesh, China, India and Pakistan under new ownership, essentially losing the essence of what Country Road stood for.' \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2637a436-883a-4a47-a0a0-52e18cfbb42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Country Road is fighting for its future largely thanks to a cost-cutting decision the company made more than 20 years ago, an expert says. The once-beloved Aussie brand is in clear trouble, with Country Road Group reporting in March its earnings were down almost 72 per cent at $14.2million for the last half of 2024.\\n\\nOne of its longstanding flagship stores at Sydney CBD's Queen Victoria Building has shut up shop, as has sister brand Trenery in Mosman, on Sydney's affluent lower north shore. Another CBD store in Sydney's Pitt Street Mall is expected to close when its lease expires in three years' time.\\n\\nThe video player is currently playing an ad. You can skip the ad in 5 sec with a mouse or keyboard\\nQueensland University of Technology marketing expert Gary Mortimer said Country Road had lost its iconic Australian lifestyle brand status since Woolworths Holdings took a controlling stake in the late 90s.\\n\\nA cost-cutting move to manufacture offshore gradually eroded its 'Made in Australia' appeal and weakened the brand's authenticity, Professor Mortimer said.\\n\\n'Since its launch in the mid-1970s, Country Road clothing was primarily made in Australia, specifically, the iconic chambray shirt which I and nearly every other young man had during that time was made here using Australian cotton,' he said.\\n\\n'The company emphasised its commitment to Australian manufacturing during that time. \\n\\n'Much of that production has shifted to Bangladesh, China, India and Pakistan under new ownership, essentially losing the essence of what Country Road stood for.' \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f926d0d-8d73-4d6f-913c-49b0a1f29460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Road's future is in clear trouble thanks to cost-cutting move to manufacture overseas\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "347c8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Road's iconic brand is in clear trouble thanks to cost-cutting\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420d210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Made in Australia' brand is in trouble thanks to cost-cutting decision 20 years ago\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "809f7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The truth about Country Road: How it has lost its Aussie identity... thanks to a cost-cutting decision 20 years ago\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac19ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Road is in clear trouble thanks to cost-cutting decision the company made 20 years ago\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f311c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab117df-022d-4715-bd95-48bb1895d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait title: Country Road is fighting for its future largely thanks to a cost-cutting decision the company made more than 20 years ago, an expert says. The once-beloved Aussie brand is in clear trouble, with Country Road Group reporting in March its earnings were down almost 72 per cent\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait_baseline(article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a89e6fb-fbf6-470d-bfb0-5665e0e49a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "\"Jessie J is reportedly planning to mark her TV comeback with a special appearance on Strictly Come Dancing this September following her mastectomy.\n",
    "\n",
    "The singer, 37, had surgery two weeks ago and confirmed to her fans that 'no cancer spread', after revealing she had been diagnosed with breast cancer in June.\n",
    "\n",
    "According to The Sun, BBC bosses believe she will kick off the new series perfectly as they welcome her return.\n",
    "\n",
    "A source said: 'Jessie is putting her health and recovery first but is already excited for getting back on stage.\n",
    "\n",
    "'She’s been booked for a TV comeback on Strictly in September and bosses can’t wait to have her on the show.\n",
    "\n",
    "'Right now she’s feeling very grateful for life and all the opportunities coming her way.'\n",
    "\n",
    "Jessie J, 37, is reportedly planning to mark her TV comeback with a special appearance on Strictly Come Dancing this September following her mastectomy (pictured in June)\n",
    "\n",
    "The singer had surgery two weeks ago and confirmed to her fans that 'no cancer spread', after revealing she had been diagnosed with breast cancer in June\n",
    "\n",
    "MailOnline has contacted Strictly and Jessie J's representatives for comment.\n",
    "\n",
    "As well as the reported Strictly appearance, Jessie is also booked to perform at Radio 2 In The Park in September in Chelmsford.\n",
    "\n",
    "Earlier this month, Jessie shared an adorable clip of her two-year-old son Sky the night before the operation.\n",
    "\n",
    "In the sweet clip, Jessie is heard encouraging her boy to say 'Mummy's going to be okay' as she prepared for the surgery to remove her breast.\n",
    "\n",
    "In the caption, she penned: 'AND.. I AM OK. Results = I have NO cancer spread. Happy tears are real', followed by several crying emojis.\n",
    "\n",
    "The Price Tag hitmaker continued: 'Thank YOU for the prayers, the love, the well wishes, the joy and all the positive energy.\n",
    "\n",
    "'This video is from the night before my surgery. We called it baby boy. You are my biggest ray of light and with you in my life, the darkness will never win.\n",
    "\n",
    "'Lots of healing to go and one more surgery to make these cousins look more like sisters, but for now it's gratitude time and I am changing my name to The LopJess monster.'\n",
    "\n",
    "BBC bosses believe she will kick off the new series perfectly as they welcome her return (pictured hosts Claudia Winkleman and Tess Daly)\n",
    "\n",
    "It comes after Jessie revealed she wished she said goodbye to her breast before undergoing a mastectomy.\n",
    "\n",
    "In an update last month, she said that she is experiencing some 'delayed sadness' and felt 'disappointed' that she didn't say goodbye.\n",
    "\n",
    "Taking to her Instagram stories, she shared a candid post with fans, bravely opening up about her feelings.\n",
    "\n",
    "The performer admitted that she went into 'survival mode' when she first found out about her diagnosis and is now letting herself be angry and sad.\n",
    "\n",
    "In her post, Jessie wrote: '2 weeks post surgery. Had my drain out 2 nights ago. She said breathe in and take a hard breathe out. She whipped that thang out so quick. Woii oii. Weirdest feeling. But so nice to have it out after 12 days.\n",
    "\n",
    "'Now it’s just me and my wonky boobs trying to figure out how to dress until I match them up. The left one is looking at me like \"\"you ok babe?\"\"'\n",
    "\n",
    "She continued: 'Also my experience was when I was diagnosed I went into survival mode.\n",
    "\n",
    "'There was so much going on with appointments and in my mind esp with a toddler I had just moved and was about to start this campaign after 8 years away. Mad.\n",
    "\n",
    "Earlier this month, Jessie shared an adorable clip of her two-year-old son Sky the night before the operation\n",
    "\n",
    "In the caption, she penned: 'AND.. I AM OK. Results = I have NO cancer spread. Happy tears are real', followed by several crying emojis\n",
    "\n",
    "'I didn’t really have a lot of time to process what was happening or what was going to happen. So I’m currently experiencing some delayed sadness and frustration by having time to process what IS happening.\n",
    "\n",
    "Jessie added: 'A little disappointed in myself I didn’t say goodbye to my old boob enough. Sounds silly but that’s where I’m at. Again that’s my journey. I’m sure others feel different.\n",
    "\n",
    "'But for me I didn’t think beyond the surgery. I was just being strong. Well now I’m here and letting myself be angry and sad and all the things. Just for a few days.\n",
    "\n",
    "'Then I will sew some padding in a bra to even them out order some t-shirts and crack tf on'.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719e4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie J set to mark TV comeback following breast cancer diagnosis following 'no cancer spread'\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7040d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie J 'will mark TV comeback with a special appearance on Strictly' after breast cancer diagnosis\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "758b3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie J, 37, plans to mark her TV comeback with a special appearance on Strictly Come Dancing following breast cancer diagnosis\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f7eb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC bosses convinced Jessie J to stage dramatic TV comeback with a special Strictly appearance following her mastectomy\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17849557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie J 'is planning to mark her TV comeback with a special appearance on Strictly' following breast cancer diagnosis\n"
     ]
    }
   ],
   "source": [
    "# Aug 21\n",
    "print(generate_clickbait(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jessie J 'set for huge TV comeback' following successful mastectomy amid breast cancer diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d037302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842bea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed700423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait title: 0000000000000000\"Jessie J is reportedly planning to mark her TV comeback with a special appearance on Strictly Come Dancing this September following her mastectomy.The singer, 37, had surgery two weeks ago and confirmed to her fans that 'no cancer spread', after revealing she had\n"
     ]
    }
   ],
   "source": [
    "print(generate_clickbait_baseline(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae5d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbait title: Kate Middleton appeared at Wimbledon with her daughter Charlotte and caught fans by surprise.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load original (unfined) model\n",
    "model_ckpt = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "def generate_clickbait_baseline(article_text):\n",
    "    prefix = \"clickbait title: \"  # Only needed if your fine-tuned model used a prefix\n",
    "    input_text = prefix + article_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=64, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example\n",
    "test_article = \"Kate Middleton appeared at Wimbledon with her daughter Charlotte and caught fans by surprise.\"\n",
    "print(generate_clickbait_baseline(test_article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90378efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_clickbait_baseline(test_article))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Clickbait ✅ REAL ONE)",
   "language": "python",
   "name": "clickbait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
